{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRI T1 maps segmentation algorithm (V1.0)\n",
    "Load packages and initialze global variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from T1maps_CImage import CImage\n",
    "import re\n",
    "import random\n",
    "\n",
    "dataset_file = \"sources_files_625_T1maps.xlsx\"\n",
    "result_rootpath = \"..\\\\results\\\\\"\n",
    "\n",
    "dim = 512\n",
    "start_epoch = 5\n",
    "inc_epoch = 5\n",
    "num_epochs = 75\n",
    "\n",
    "_batchsize = 16\n",
    "\n",
    "metrics = [\"jacc_metric\", \"dice_metric\"]\n",
    "activations = [\"LeakyReLU\", \"ReLU\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader, file handling and helper routines\n",
    "\n",
    "- load_shuffeled_images_and_masks()\n",
    "- plotMask()\n",
    "- create_excel_file()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===========================================================================================\n",
    "@fn         load_shuffeled_images_and_masks()\n",
    "@details    First, load dataframe containing all image and mask paths and second, load dicom images and .mha masks from disk   \n",
    "@param[in]  file_matrix - dataframe with patient list an file paths\n",
    "@param[in]  X_shape - target image dimension \n",
    "@return     [train_dcms, train_masks, test_dcms, test_masks, val_dcms, val_masks] - Lists of images and masks \n",
    "@note  \n",
    "===========================================================================================\n",
    "\"\"\"  \n",
    "def load_shuffeled_images_and_masks(file_matrix, X_shape):\n",
    "\n",
    "    train_dcms = []\n",
    "    train_masks = []\n",
    "    test_dcms = []\n",
    "    test_masks = []\n",
    "    val_dcms = []\n",
    "    val_masks = []\n",
    "\n",
    "    size = file_matrix.shape\n",
    "    num_data = size[0]\n",
    "\n",
    "    # randomize index list for later data set splitting\n",
    "    rand_idx = random.sample(list(range(num_data)), num_data)\n",
    "    \n",
    "    ## Split list into two sections 90% and 10%\n",
    "    def split_two(lst, ratio=[0.9, 0.1]):\n",
    "        assert(np.sum(ratio) == 1.0)  # makes sure the splits make sense\n",
    "        train_ratio = ratio[0]\n",
    "        # note this function needs only the \"middle\" index to split, the remaining is the rest of the split\n",
    "        indices_for_splittin = [int(len(lst) * train_ratio)]\n",
    "        train, test = np.split(lst, indices_for_splittin)\n",
    "        return train, test\n",
    "\n",
    "    ## Split list into three sections 80%, 10% and 10%\n",
    "    def split_three(lst, ratio=[0.8, 0.1, 0.1]):\n",
    "        train_r, val_r, test_r = ratio\n",
    "        assert(np.sum(ratio) == 1.0)  # makes sure the splits make sense\n",
    "        # note we only need to give the first 2 indices to split, the last one it returns the rest of the list or empty\n",
    "        indicies_for_splitting = [int(len(lst) * train_r), int(len(lst) * (train_r+val_r))]\n",
    "        train, val, test = np.split(lst, indicies_for_splitting)\n",
    "        return train, val, test\n",
    "\n",
    "    ## Create random fractional index lists.\n",
    "    train_idx, val_idx, test_idx = split_three(rand_idx)\n",
    "    print(\"Train-List: \")\n",
    "    print(train_idx)\n",
    "    print(\"Val-List: \")\n",
    "    print(val_idx)\n",
    "    print(\"Test-List: \")\n",
    "    print(test_idx)\n",
    "    \n",
    "    ## Save index list for later testing.\n",
    "    # idxfile = os.path.join(result_rootpath, \"v108_indices.txt\")\n",
    "    # create_index_file(idxfile, train_idx, test_idx, val_idx)\n",
    "   \n",
    "    ## Start image reader loop from merged excel path file\n",
    "    for idx in rand_idx: \n",
    "        patidx_str = re.findall(r'\\d+', file_matrix[\"Patient\"][idx])\n",
    "        patidx = int(patidx_str[0])\n",
    "        #print(patidx)\n",
    "    \n",
    "        mha_idx = file_matrix[\"MHA_INDEX\"][idx]\n",
    "        dcm_img = CImage(file_matrix[\"DICOM\"][idx], \"DCM_DUMMY\", 'image', \".dcm\")\n",
    "        mha_mask = CImage(file_matrix[\"MHA_MASK\"][idx], \"MHA_MASK\", 'mask', \".mha\")\n",
    "\n",
    "        mha_mask_data = mha_mask.img[:,:,mha_idx]\n",
    "        if mha_mask.imagesize[0] != dcm_img.imagesize[0] and mha_mask.imagesize[1] != dcm_img.imagesize[1]:\n",
    "            dcm_img_data = dcm_img.imgT    \n",
    "        else:\n",
    "            dcm_img_data = dcm_img.img    \n",
    "\n",
    "        dcm_mod = cv2.resize(dcm_img_data,(X_shape,X_shape), interpolation=cv2.INTER_NEAREST)[:,:]\n",
    "        dcm_mod = dcm_mod.astype(np.uint16)\n",
    "\n",
    "        mask_mod = cv2.resize(mha_mask_data,(X_shape,X_shape), interpolation=cv2.INTER_NEAREST)[:,:]\n",
    "        mask_mod16 = (mask_mod - mask_mod.min()) / (mask_mod.max() - mask_mod.min()) * 4095\n",
    "        mask_mod16 = mask_mod16.astype(np.uint16)\n",
    "        \n",
    "        if idx in train_idx:\n",
    "            train_dcms.append(dcm_mod)\n",
    "            train_masks.append(mask_mod16)\n",
    "        elif idx in test_idx:\n",
    "            test_dcms.append(dcm_mod)\n",
    "            test_masks.append(mask_mod16)\n",
    "        elif idx in val_idx:\n",
    "            val_dcms.append(dcm_mod)\n",
    "            val_masks.append(mask_mod16)\n",
    "\n",
    "    return [train_dcms, train_masks, test_dcms, test_masks, val_dcms, val_masks] \n",
    "\n",
    "\"\"\"\n",
    "===========================================================================================\n",
    "@fn         plotMask()\n",
    "@details    Display function for source images and reference masks\n",
    "@param[in]  X -source images\n",
    "@param[in]  y - reference masks \n",
    "@return     void\n",
    "@note  \n",
    "\"\"\" \n",
    "def plotMask(X,y):\n",
    "    sample = []\n",
    "    \n",
    "    for i in range(6):\n",
    "        left = X[i]\n",
    "        right = y[i]\n",
    "        combined = np.hstack((left,right))\n",
    "        sample.append(combined)\n",
    "        \n",
    "    for i in range(0,6,3):\n",
    "\n",
    "        plt.figure(figsize=(25,10))\n",
    "        \n",
    "        plt.subplot(2,3,1+i)\n",
    "        plt.imshow(sample[i],'gray')\n",
    "        \n",
    "        plt.subplot(2,3,2+i)\n",
    "        plt.imshow(sample[i+1],'gray')\n",
    "                \n",
    "        plt.subplot(2,3,3+i)\n",
    "        plt.imshow(sample[i+2],'gray')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\"\"\"\n",
    "===========================================================================================\n",
    "@fn         create_excel_file()\n",
    "@details    Creates dataframe of model arruracies and writes to excel  \n",
    "@return     void\n",
    "@note  \n",
    "\"\"\" \n",
    "def create_excel_file(filepath, loss_arr, val_loss_arr, acc_arr, val_acc_arr, sheet):\n",
    "    df = pd.DataFrame({'Loss': loss_arr,\n",
    "                       'Validation Loss': val_loss_arr,\n",
    "                       'Accuracy': acc_arr,\n",
    "                       'Validation Accuracy': val_acc_arr})\n",
    "    writer = pd.ExcelWriter(filepath, engine='xlsxwriter')\n",
    "    # Convert the dataframe to an XlsxWriter Excel object.\n",
    "    df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets[sheet]\n",
    "\n",
    "    # Add a header format.\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#D7E4BC',\n",
    "        'border': 1})\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics, loss functions and model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "from keras.layers import LeakyReLU\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "\"\"\"\n",
    "===========================================================================================\n",
    "@fn         dice_coef()\n",
    "@details    DSC calculation function  \n",
    "@param[in]  y_true - Set of reference mask\n",
    "@param[in]  y_pred - Set of prediction mask\n",
    "@return     Dice Similay Coefficient \n",
    "@note  \n",
    "\"\"\" \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "\n",
    "\"\"\"\n",
    "===========================================================================================\n",
    "@fn         dice_coef_loss()\n",
    "@details    Dice loss function  \n",
    "@param[in]  y_true - Set of reference mask\n",
    "@param[in]  y_pred - Set of prediction mask\n",
    "@return     Dice Loss \n",
    "@note  \n",
    "\"\"\" \n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "\"\"\"\n",
    "===========================================================================================\n",
    "@fn         jaccard_coef()\n",
    "@details    Creates dataframe of model arruracies and writes to excel  \n",
    "@param[in]  y_true - Set of reference mask\n",
    "@param[in]  y_pred - Set of prediction mask\n",
    "@return     Dice Similay Coefficient \n",
    "@note  \n",
    "\"\"\" \n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\"\"\"\n",
    "===========================================================================================\n",
    "@fn         jaccard_coef_loss()\n",
    "@details    IOU loss function  \n",
    "@param[in]  y_true - Set of reference mask\n",
    "@param[in]  y_pred - Set of prediction mask\n",
    "@return     IOU Loss \n",
    "@note  \n",
    "\"\"\" \n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return 1-jaccard_coef(y_true, y_pred)  \n",
    "\n",
    "\"\"\"\n",
    "===========================================================================================\n",
    "@fn         unet()\n",
    "@details    Creates dataframe of model arruracies and writes to excel  \n",
    "@author     MM\n",
    "@date       \n",
    "@return     model\n",
    "@note  \n",
    "\"\"\"\n",
    "def unet(_activation, _metric, input_size=(256,256,1)):\n",
    "    if _activation == \"ReLU\":\n",
    "        inputs = Input(input_size)\n",
    "        \n",
    "        conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "        conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "        conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "        conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "        conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "        conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "        conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "        conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "        conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "        up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "        conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "        conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "        up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "        conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "        conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "        up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "        conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "        conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "        up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "        conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "        conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "        _outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    elif _activation == \"LeakyReLU\":\n",
    "        inputs = Input(input_size)\n",
    "        \n",
    "        conv1 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(inputs)\n",
    "        conv1 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "        conv2 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(pool1)\n",
    "        conv2 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "        conv3 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(pool2)\n",
    "        conv3 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "        conv4 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(pool3)\n",
    "        conv4 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "        conv5 = Conv2D(512, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(pool4)\n",
    "        conv5 = Conv2D(512, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv5)\n",
    "\n",
    "        up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "        conv6 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(up6)\n",
    "        conv6 = Conv2D(256, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv6)\n",
    "\n",
    "        up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "        conv7 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(up7)\n",
    "        conv7 = Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv7)\n",
    "\n",
    "        up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "        conv8 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(up8)\n",
    "        conv8 = Conv2D(64, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv8)\n",
    "\n",
    "        up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "        conv9 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(up9)\n",
    "        conv9 = Conv2D(32, (3, 3), activation=LeakyReLU(alpha=0.1), padding='same')(conv9)\n",
    "\n",
    "        _outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    _model = Model(inputs=[inputs], outputs=[_outputs])\n",
    "\n",
    "    if _metric == \"dice_metric\":      \n",
    "        _model.compile(optimizer=Adam(learning_rate=1e-5), loss=dice_coef_loss,metrics=[dice_coef, 'binary_accuracy'])\n",
    "    elif _metric == \"jacc_metric\": \n",
    "        _model.compile(optimizer=Adam(learning_rate=1e-5), loss=[jaccard_coef_loss], metrics = [jaccard_coef, 'binary_accuracy'])\n",
    "\n",
    "    _model.summary()\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbd0220555c692baee8ea509403d64364e2ae439"
   },
   "source": [
    "# Load required data and perform the training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from keras.optimizers import Adam \n",
    "\n",
    "## step 1\n",
    "_file_matrix = pd.read_excel(dataset_file , sheet_name=\"matches\")\n",
    "[X_train, y_train, X_test, y_test, X_val, y_val,] = load_shuffeled_images_and_masks(_file_matrix, dim)\n",
    "\n",
    "print(\"Num Training-Dataset: %d\" % len(X_train))\n",
    "print(\"Num Training-Masks: %d\" % len(y_train))\n",
    "print(\"Num Test-Dataset: %d\" % len(X_test))\n",
    "print(\"Num Test-Masks: %d\" % len(y_test))\n",
    "print(\"Num Val-Dataset: %d\" % len(X_val))\n",
    "print(\"Num Val-Masks: %d\" % len(y_val))\n",
    "\n",
    "print(\"training set\")\n",
    "plotMask(X_train,y_train)\n",
    "print(\"testing set\")\n",
    "plotMask(X_test,y_test)\n",
    "\n",
    "## step 2 - Cast image and masks lists to numpy array  \n",
    "X_train = np.array(X_train).reshape(len(X_train),dim,dim,1)\n",
    "y_train = np.array(y_train).reshape(len(y_train),dim,dim,1)\n",
    "X_test = np.array(X_test).reshape(len(X_test),dim,dim,1)\n",
    "y_test = np.array(y_test).reshape(len(y_test),dim,dim,1)\n",
    "X_val = np.array(X_val).reshape(len(X_val),dim,dim,1)\n",
    "y_val = np.array(y_val).reshape(len(y_val),dim,dim,1)\n",
    "assert X_train.shape == y_train.shape\n",
    "assert X_test.shape == y_test.shape\n",
    "assert X_val.shape == y_val.shape\n",
    "\n",
    "## step 3 - center value range of datasets to zero within [-1, 1]\n",
    "train_vol = (X_train-2048.0)/2048.0\n",
    "validation_vol = (X_val-2048.0)/2048.0\n",
    "test_vol = (X_test-2048.0)/2048.0\n",
    "\n",
    "train_seg = (y_train>2048).astype(np.float32)\n",
    "validation_seg = (y_val>2048).astype(np.float32)\n",
    "test_seg = (y_test>2048).astype(np.float32)\n",
    "\n",
    "## step 4 - Start training loop\n",
    "for epochs in range(start_epoch, num_epochs, inc_epoch):\n",
    "    for _activation in activations:\n",
    "        for _metric in metrics:\n",
    "            print(\"Training mit %d Epochen:\"%(epochs))\n",
    "            # Create resultfolder and Excel file\n",
    "            resultdir = \"%d_epochs_%s_%s_%dbatch\\\\\"%(epochs,_metric, _activation,_batchsize)\n",
    "            resultpath = os.path.join(result_rootpath, resultdir)\n",
    "            try: \n",
    "                os.mkdir(resultpath) \n",
    "                print(\"Directory '%s' created\" % resultpath)\n",
    "            except OSError as error: \n",
    "                print(error) \n",
    "\n",
    "            ## Compile and train the U-Net Model\n",
    "            model = unet(_activation, _metric, input_size=(512,512,1))\n",
    "            \n",
    "            from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "            weight_file=\"myocardseg_model_weights%depoch_%s_%s.best.hdf5\"%(epochs, _activation, _metric)\n",
    "            weight_path = os.path.join(resultpath, weight_file) \n",
    "\n",
    "            ## Callbacks, Early Stopping and Reduced LR\n",
    "            checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only = True)\n",
    "            reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min', min_delta=0.0001, cooldown=2, min_lr=1e-6)\n",
    "\n",
    "            with_early = False\n",
    "            if with_early:\n",
    "                early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15) \n",
    "                callbacks_list = [checkpoint, early_stopping, reduceLROnPlat]\n",
    "            else:\n",
    "                callbacks_list = [checkpoint, reduceLROnPlat]\n",
    "\n",
    "            ## TRAIN the model\n",
    "            if _metric == \"dice_metric\":      \n",
    "                model.compile(optimizer=Adam(learning_rate=2e-4), loss=[dice_coef_loss], metrics = [dice_coef, 'binary_accuracy'])\n",
    "            elif _metric == \"jacc_metric\": \n",
    "                model.compile(optimizer=Adam(learning_rate=2e-4), loss=[jaccard_coef_loss], metrics = [jaccard_coef, 'binary_accuracy'])\n",
    "\n",
    "            model.reset_states()\n",
    "            loss_history = model.fit(   x=train_vol, \n",
    "                                        y=train_seg, \n",
    "                                        batch_size=_batchsize, \n",
    "                                        epochs=epochs, \n",
    "                                        validation_data=(test_vol,test_seg), \n",
    "                                        callbacks=callbacks_list)\n",
    "            print(loss_history)\n",
    "\n",
    "            ## Clear output in notebook\n",
    "            clear_output()\n",
    "\n",
    "            ## Plot metric and evaluate\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "            ax1.plot(loss_history.history['loss'], '-', label = 'Loss')\n",
    "            ax1.plot(loss_history.history['val_loss'], '-', label = 'Validation Loss')\n",
    "            ax1.set_xlabel(\"Epoch\")\n",
    "            ax1.legend()\n",
    "            ax1.grid()\n",
    "\n",
    "            ax2.plot(100*np.array(loss_history.history['binary_accuracy']), '-', label = 'Accuracy')\n",
    "            ax2.plot(100*np.array(loss_history.history['val_binary_accuracy']), '-', label = 'Validation Accuracy')\n",
    "            ax2.set_xlabel(\"Epoch\")\n",
    "            ax2.legend()\n",
    "            ax2.grid()\n",
    "            filename = \"HR_%dEpochs_Accuracy_%s_%s.png\"%(epochs, _activation, _metric)\n",
    "            figurepath = os.path.join(resultpath, filename) \n",
    "            fig.savefig(figurepath, dpi=600)\n",
    "\n",
    "            ## Save accuracy results to excel\n",
    "            excelfile = \"%dEpochs_accuarcy_%s_%s.xlsx\"%(epochs, _activation, _metric)\n",
    "            excelpath = os.path.join(resultpath,excelfile)\n",
    "            try:\n",
    "                create_excel_file(excelpath, np.array(loss_history.history['loss']), np.array(loss_history.history['val_loss']), np.array(loss_history.history['binary_accuracy']), np.array(loss_history.history['val_binary_accuracy']), \"val_acc\")\n",
    "                print(\"File '%s' was created\" % excelfile)\n",
    "            except OSError as error: \n",
    "                print(error) \n",
    "\n",
    "            ## Test the model with validation dataset and plot results\n",
    "            pred_candidates = np.random.randint(1,validation_vol.shape[0],10)\n",
    "\n",
    "            plt.figure(figsize=(16,10))\n",
    "            pred_index = np.arange(0,validation_vol.shape[0],1)\n",
    "            print(pred_index)\n",
    "            print(pred_index.shape[0])\n",
    "           \n",
    "            preds = model.predict(validation_vol)\n",
    "\n",
    "            for i in pred_index:\n",
    "                plt.figure(figsize=(16,5))\n",
    "                plt.subplot(1,3,1)\n",
    "                    \n",
    "                plt.imshow(validation_vol[i],'gray', interpolation='none')\n",
    "                plt.xlabel(\"MRI T1-Map\")\n",
    "                    \n",
    "                plt.subplot(1,3,2)\n",
    "                plt.imshow(validation_seg[i],'gray', interpolation='none')\n",
    "                plt.xlabel(\"Ground Truth\")\n",
    "                    \n",
    "                plt.subplot(1,3,3)\n",
    "                plt.imshow(preds[i],'gray', interpolation='none')\n",
    "                plt.xlabel(\"Prediction\")\n",
    "            \n",
    "                filename = \"%d_Epochs_-Ipol_%s_%s_%dbatch_%s_Segm%d.png\"%(epochs,_metric, _activation,_batchsize,_metric,i)\n",
    "                figurepath = os.path.join(resultpath, filename) \n",
    "                plt.savefig(figurepath, dpi=300)\n",
    "                plt.cla()\n",
    "                clear_output()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
